{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "waRmTZ3Ma98B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57454b47-eb72-4e98-edee-9fbc62981415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tiFdxwAHcfJb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "0fM1vv4eci0N",
        "outputId": "b87483ba-d0dd-465e-a8a8-a8d32f7a213c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-54d0a58e-4eef-4915-9e6b-b57b47c4cd6e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-54d0a58e-4eef-4915-9e6b-b57b47c4cd6e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"regidor24\",\"key\":\"c0a94ad325e7da67b7774933fadf51fc\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v9g0I3mAcmCJ"
      },
      "outputs": [],
      "source": [
        "!mv kaggle.json /root/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GZzwtXkucpyD"
      },
      "outputs": [],
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "McYC6kE7cq0M",
        "outputId": "1d79c2d2-2a68-4435-d202-98f752ab7b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/domingosnhamussusa/earthquake-damage-in-nepal\n",
            "License(s): unknown\n",
            "Downloading earthquake-damage-in-nepal.zip to /content\n",
            "  0% 0.00/7.78M [00:00<?, ?B/s]\n",
            "100% 7.78M/7.78M [00:00<00:00, 1.07GB/s]\n",
            "Dataset ready!\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d domingosnhamussusa/earthquake-damage-in-nepal\n",
        "\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/earthquake-damage-in-nepal.zip', 'r')\n",
        "zip_ref.extractall('/content/earthquake_data')\n",
        "zip_ref.close()\n",
        "\n",
        "print(\"Dataset ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "collapsed": true,
        "id": "QtTPSXFVctyc",
        "outputId": "f038823e-d190-4156-a94e-1de7d42105b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    building_id  district_id  count_floors_pre_eq  count_floors_post_eq  \\\n",
              "0  120101000011           12                    1                     1   \n",
              "1  120101000021           12                    1                     1   \n",
              "2  120101000031           12                    1                     1   \n",
              "3  120101000041           12                    1                     1   \n",
              "4  120101000051           12                    1                     1   \n",
              "\n",
              "   age_building  plinth_area_sq_ft  height_ft_pre_eq  height_ft_post_eq  \\\n",
              "0             9                288                 9                  9   \n",
              "1            15                364                 9                  9   \n",
              "2            20                384                 9                  9   \n",
              "3            20                312                 9                  9   \n",
              "4            30                308                 9                  9   \n",
              "\n",
              "  land_surface_condition foundation_type                 roof_type  \\\n",
              "0                   Flat           Other  Bamboo/Timber-Light roof   \n",
              "1                   Flat           Other  Bamboo/Timber-Light roof   \n",
              "2                   Flat           Other  Bamboo/Timber-Light roof   \n",
              "3                   Flat           Other  Bamboo/Timber-Light roof   \n",
              "4                   Flat           Other  Bamboo/Timber-Light roof   \n",
              "\n",
              "  ground_floor_type other_floor_type      position plan_configuration  \\\n",
              "0               Mud   Not applicable  Not attached        Rectangular   \n",
              "1               Mud   Not applicable  Not attached        Rectangular   \n",
              "2               Mud   Not applicable  Not attached        Rectangular   \n",
              "3               Mud   Not applicable  Not attached        Rectangular   \n",
              "4               Mud   Not applicable  Not attached        Rectangular   \n",
              "\n",
              "           condition_post_eq damage_grade  \n",
              "0       Damaged-Used in risk      Grade 3  \n",
              "1  Damaged-Repaired and used      Grade 5  \n",
              "2  Damaged-Repaired and used      Grade 2  \n",
              "3  Damaged-Repaired and used      Grade 2  \n",
              "4  Damaged-Repaired and used      Grade 1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da59f6f8-8b28-47b3-95e8-4bba36751e61\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>building_id</th>\n",
              "      <th>district_id</th>\n",
              "      <th>count_floors_pre_eq</th>\n",
              "      <th>count_floors_post_eq</th>\n",
              "      <th>age_building</th>\n",
              "      <th>plinth_area_sq_ft</th>\n",
              "      <th>height_ft_pre_eq</th>\n",
              "      <th>height_ft_post_eq</th>\n",
              "      <th>land_surface_condition</th>\n",
              "      <th>foundation_type</th>\n",
              "      <th>roof_type</th>\n",
              "      <th>ground_floor_type</th>\n",
              "      <th>other_floor_type</th>\n",
              "      <th>position</th>\n",
              "      <th>plan_configuration</th>\n",
              "      <th>condition_post_eq</th>\n",
              "      <th>damage_grade</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>120101000011</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>288</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>Flat</td>\n",
              "      <td>Other</td>\n",
              "      <td>Bamboo/Timber-Light roof</td>\n",
              "      <td>Mud</td>\n",
              "      <td>Not applicable</td>\n",
              "      <td>Not attached</td>\n",
              "      <td>Rectangular</td>\n",
              "      <td>Damaged-Used in risk</td>\n",
              "      <td>Grade 3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>120101000021</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>364</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>Flat</td>\n",
              "      <td>Other</td>\n",
              "      <td>Bamboo/Timber-Light roof</td>\n",
              "      <td>Mud</td>\n",
              "      <td>Not applicable</td>\n",
              "      <td>Not attached</td>\n",
              "      <td>Rectangular</td>\n",
              "      <td>Damaged-Repaired and used</td>\n",
              "      <td>Grade 5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>120101000031</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>384</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>Flat</td>\n",
              "      <td>Other</td>\n",
              "      <td>Bamboo/Timber-Light roof</td>\n",
              "      <td>Mud</td>\n",
              "      <td>Not applicable</td>\n",
              "      <td>Not attached</td>\n",
              "      <td>Rectangular</td>\n",
              "      <td>Damaged-Repaired and used</td>\n",
              "      <td>Grade 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>120101000041</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>312</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>Flat</td>\n",
              "      <td>Other</td>\n",
              "      <td>Bamboo/Timber-Light roof</td>\n",
              "      <td>Mud</td>\n",
              "      <td>Not applicable</td>\n",
              "      <td>Not attached</td>\n",
              "      <td>Rectangular</td>\n",
              "      <td>Damaged-Repaired and used</td>\n",
              "      <td>Grade 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>120101000051</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>308</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>Flat</td>\n",
              "      <td>Other</td>\n",
              "      <td>Bamboo/Timber-Light roof</td>\n",
              "      <td>Mud</td>\n",
              "      <td>Not applicable</td>\n",
              "      <td>Not attached</td>\n",
              "      <td>Rectangular</td>\n",
              "      <td>Damaged-Repaired and used</td>\n",
              "      <td>Grade 1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da59f6f8-8b28-47b3-95e8-4bba36751e61')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-da59f6f8-8b28-47b3-95e8-4bba36751e61 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-da59f6f8-8b28-47b3-95e8-4bba36751e61');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ddde6ab6-7e00-48ae-b3e4-42a4c4020fde\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ddde6ab6-7e00-48ae-b3e4-42a4c4020fde')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ddde6ab6-7e00-48ae-b3e4-42a4c4020fde button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/earthquake_data/NepalEarhquakeDamage2015.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "l-uIL4xAZRca",
        "outputId": "62333025-b985-4bf8-bd18-916b425251a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns dropped: ['building_id', 'district_id', 'count_floors_post_eq', 'height_ft_post_eq', 'condition_post_eq']\n",
            "Shape after dropping columns: (762106, 12)\n",
            "DataFrame after dropping rows with missing values:\n",
            "   count_floors_pre_eq  age_building  plinth_area_sq_ft  height_ft_pre_eq  \\\n",
            "0                    1             9                288                 9   \n",
            "1                    1            15                364                 9   \n",
            "2                    1            20                384                 9   \n",
            "3                    1            20                312                 9   \n",
            "4                    1            30                308                 9   \n",
            "\n",
            "  land_surface_condition foundation_type                 roof_type  \\\n",
            "0                   Flat           Other  Bamboo/Timber-Light roof   \n",
            "1                   Flat           Other  Bamboo/Timber-Light roof   \n",
            "2                   Flat           Other  Bamboo/Timber-Light roof   \n",
            "3                   Flat           Other  Bamboo/Timber-Light roof   \n",
            "4                   Flat           Other  Bamboo/Timber-Light roof   \n",
            "\n",
            "  ground_floor_type other_floor_type      position plan_configuration  \\\n",
            "0               Mud   Not applicable  Not attached        Rectangular   \n",
            "1               Mud   Not applicable  Not attached        Rectangular   \n",
            "2               Mud   Not applicable  Not attached        Rectangular   \n",
            "3               Mud   Not applicable  Not attached        Rectangular   \n",
            "4               Mud   Not applicable  Not attached        Rectangular   \n",
            "\n",
            "  damage_grade  \n",
            "0      Grade 3  \n",
            "1      Grade 5  \n",
            "2      Grade 2  \n",
            "3      Grade 2  \n",
            "4      Grade 1  \n",
            "Shape after handling missing values: (762094, 12)\n",
            "Missing values check after cleaning: 0\n"
          ]
        }
      ],
      "source": [
        "columns_to_drop = [\n",
        "    'building_id',\n",
        "    'district_id',\n",
        "    'count_floors_post_eq',\n",
        "    'height_ft_post_eq',\n",
        "    'condition_post_eq'\n",
        "]\n",
        "\n",
        "df_cleaned = df.drop(columns=columns_to_drop)\n",
        "print(f\"Columns dropped: {columns_to_drop}\")\n",
        "print(\"Shape after dropping columns:\", df_cleaned.shape)\n",
        "\n",
        "df_cleaned.dropna(inplace=True)\n",
        "print(\"DataFrame after dropping rows with missing values:\")\n",
        "print(df_cleaned.head())\n",
        "print(\"Shape after handling missing values:\", df_cleaned.shape)\n",
        "print(\"Missing values check after cleaning:\", df_cleaned.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cl9PVeKofRMq",
        "outputId": "f356a37c-e547-400f-8936-3dab01d4d38a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(762106, 17)\n",
            "building_id                0\n",
            "district_id                0\n",
            "count_floors_pre_eq        0\n",
            "count_floors_post_eq       0\n",
            "age_building               0\n",
            "plinth_area_sq_ft          0\n",
            "height_ft_pre_eq           0\n",
            "height_ft_post_eq          0\n",
            "land_surface_condition     0\n",
            "foundation_type            0\n",
            "roof_type                  0\n",
            "ground_floor_type          0\n",
            "other_floor_type           0\n",
            "position                   1\n",
            "plan_configuration         1\n",
            "condition_post_eq          0\n",
            "damage_grade              12\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2578902962.py:4: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill').fillna(method='bfill')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(df.shape)\n",
        "print(df.isnull().sum())\n",
        "\n",
        "df = df.fillna(method='ffill').fillna(method='bfill')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bFUir_l8fVio",
        "outputId": "d86f8bb5-ce99-4669-cdfa-52134d31ecb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category-encoders\n",
            "  Downloading category_encoders-2.9.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category-encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category-encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category-encoders) (1.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category-encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category-encoders) (1.16.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category-encoders) (0.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category-encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category-encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category-encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category-encoders) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category-encoders) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install category-encoders\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "data = df.copy()\n",
        "target_col = 'damage_grade'\n",
        "\n",
        "# Extract target variable 'y' and drop it from 'data' (features)\n",
        "y = data[target_col]\n",
        "data = data.drop(columns=[target_col])\n",
        "\n",
        "# Convert target labels to numerical format\n",
        "le = LabelEncoder()\n",
        "y = pd.Series(le.fit_transform(y), index=y.index)\n",
        "\n",
        "if 'age' in data.columns:\n",
        "    data['age_bucket'] = pd.cut(data['age'], bins=[-1,5,15,30,60,200], labels=['new','young','mid','old','very_old'])\n",
        "\n",
        "\n",
        "cat_cols = data.select_dtypes(include=['object','category']).columns.tolist()\n",
        "\n",
        "maybe_cat = ['foundation_type','land_surface_condition','roof_type','ground_floor_type','position']\n",
        "for c in maybe_cat:\n",
        "    if c in data.columns and data[c].nunique() < 30:\n",
        "        cat_cols.append(c)\n",
        "\n",
        "cat_cols = sorted(list(set(cat_cols)))\n",
        "# The target_col is already removed from data, so no need to filter it out here\n",
        "num_cols = [c for c in data.columns if c not in cat_cols]\n",
        "\n",
        "\n",
        "high_card_threshold = 20\n",
        "te_cols = [c for c in cat_cols if data[c].nunique() > high_card_threshold]\n",
        "ohe_cols = [c for c in cat_cols if c not in te_cols]\n",
        "\n",
        "\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "data[num_cols] = num_imputer.fit_transform(data[num_cols])\n",
        "\n",
        "\n",
        "data = pd.get_dummies(data, columns=ohe_cols, drop_first=True)\n",
        "\n",
        "\n",
        "if 'age' in data.columns and 'count_floors_pre_eq' in data.columns:\n",
        "    data['age_x_floors'] = data['age'] * data['count_floors_pre_eq']\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data[num_cols] = scaler.fit_transform(data[num_cols])\n",
        "\n",
        "\n",
        "X = data\n",
        "# y is already defined above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAtr4So6mehl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -----------------------------\n",
        "# Identify columns\n",
        "# -----------------------------\n",
        "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Handle missing values\n",
        "# -----------------------------\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
        "\n",
        "if len(cat_cols) > 0:\n",
        "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "    X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Handle outliers (IQR clipping)\n",
        "# -----------------------------\n",
        "for col in num_cols:\n",
        "    Q1 = X[col].quantile(0.25)\n",
        "    Q3 = X[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "\n",
        "    X[col] = np.clip(X[col], lower, upper)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Feature scaling\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "print(\"Missing values handled, outliers capped, features scaled safely.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bquYg8HPfaYQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "\n",
        "from category_encoders import TargetEncoder\n",
        "te = TargetEncoder(cols=te_cols)\n",
        "if te_cols:\n",
        "    X_train = X_train.copy()\n",
        "    X_test = X_test.copy()\n",
        "    X_train[te_cols] = te.fit_transform(X_train[te_cols], y_train)\n",
        "    X_test[te_cols] = te.transform(X_test[te_cols])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc6vCqkpgAe9"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "xgb_clf.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
        "\n",
        "\n",
        "lgb_clf = lgb.LGBMClassifier(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=31,\n",
        "    colsample_bytree=0.8,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "lgb_clf.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
        "\n",
        "\n",
        "for name, model in [('XGBoost', xgb_clf), ('LightGBM', lgb_clf)]:\n",
        "    preds = model.predict(X_test)\n",
        "    print(f'=== {name} ===')\n",
        "    print('Accuracy:', accuracy_score(y_test, preds))\n",
        "    print(classification_report(y_test, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gRc57SgllwUP"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3h_sTyPccFR"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "459aa6fb"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the LightGBM model\n",
        "joblib.dump(lgb_clf, 'lightgbm_model.pkl')\n",
        "\n",
        "print(\"LightGBM model saved as 'lightgbm_model.pkl'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EB1jqIKmI6N"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "loaded_model = joblib.load('lightgbm_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36d08b12"
      },
      "source": [
        "To load the model later, you can use the following code:\n",
        "```python\n",
        "import joblib\n",
        "loaded_model = joblib.load('lightgbm_model.pkl')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keyTVuzTzfUs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzJlVImnGnxo"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "joblib.dump(lgb_clf, 'lgb_model.joblib')\n",
        "joblib.dump(xgb_clf, 'xgb_model.joblib')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_LtfaIfyjxG"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "\n",
        "final_training_features = X.columns.tolist()\n",
        "\n",
        "\n",
        "joblib.dump(final_training_features, \"features.joblib\")\n",
        "\n",
        "print(f\"Saved {len(final_training_features)} features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3HIypy551b6"
      },
      "outputs": [],
      "source": [
        "final_model_features = joblib.load(\"features.joblib\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0AjS7kvNvOb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O2j7bO8vjpW"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(lgb_clf, 'lgb_model.joblib')\n",
        "joblib.dump(xgb_clf, 'xgb_model.joblib')\n",
        "\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "joblib.dump(num_imputer, 'num_imputer.joblib')\n",
        "joblib.dump(te, 'target_encoder.joblib')\n",
        "joblib.dump(le, 'label_encoder.joblib')\n",
        "joblib.dump(num_cols, 'num_cols.joblib')\n",
        "joblib.dump(ohe_cols, 'ohe_cols.joblib')\n",
        "joblib.dump(te_cols, 'te_cols.joblib')\n",
        "joblib.dump(X_train.columns.tolist(), 'final_training_columns.joblib')\n",
        "\n",
        "print(\"Models & preprocessors saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQRgaU_5vxX5"
      },
      "outputs": [],
      "source": [
        "def preprocess_input(df, training=False):\n",
        "    import joblib\n",
        "    import pandas as pd\n",
        "\n",
        "    scaler = joblib.load('scaler.joblib')\n",
        "    num_imputer = joblib.load('num_imputer.joblib')\n",
        "    te = joblib.load('target_encoder.joblib')\n",
        "    num_cols = joblib.load('num_cols.joblib')\n",
        "    ohe_cols = joblib.load('ohe_cols.joblib')\n",
        "    te_cols = joblib.load('te_cols.joblib')\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    if 'age' in df.columns:\n",
        "        df['age_bucket'] = pd.cut(\n",
        "            df['age'],\n",
        "            bins=[-1,5,15,30,60,200],\n",
        "            labels=['new','young','mid','old','very_old']\n",
        "        )\n",
        "\n",
        "    df[num_cols] = num_imputer.transform(df[num_cols])\n",
        "\n",
        "    df = pd.get_dummies(df, columns=ohe_cols, drop_first=True)\n",
        "\n",
        "    for col in te_cols:\n",
        "        if col not in df.columns:\n",
        "            df[col] = \"Unknown\"\n",
        "\n",
        "    df[te_cols] = te.transform(df[te_cols])\n",
        "\n",
        "    for col in scaler.feature_names_in_:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 0\n",
        "\n",
        "    df = df[scaler.feature_names_in_]\n",
        "    df[num_cols] = scaler.transform(df[num_cols])\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQRgcIyyv5lE"
      },
      "outputs": [],
      "source": [
        "def predict_damage(input_df):\n",
        "    import joblib\n",
        "\n",
        "    model = joblib.load('lgb_model.joblib')\n",
        "    label_encoder = joblib.load('label_encoder.joblib')\n",
        "\n",
        "    processed = preprocess_input(input_df)\n",
        "    pred = model.predict(processed)\n",
        "    prob = model.predict_proba(processed)\n",
        "\n",
        "    damage_label = label_encoder.inverse_transform(pred)[0]\n",
        "    confidence = prob.max()\n",
        "\n",
        "    return damage_label, confidence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kjKdUtAKPdWq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def statistical_summary(df):\n",
        "    summary = pd.DataFrame({\n",
        "        \"Missing Values\": df.isnull().sum(),\n",
        "        \"Missing %\": (df.isnull().mean() * 100).round(2),\n",
        "        \"Min\": df.min(numeric_only=True),\n",
        "        \"Max\": df.max(numeric_only=True),\n",
        "        \"Median\": df.median(numeric_only=True),\n",
        "        \"Mean\": df.mean(numeric_only=True),\n",
        "        \"Std Dev\": df.std(numeric_only=True)\n",
        "    })\n",
        "    return summary\n",
        "\n",
        "summary_table = statistical_summary(df_cleaned)\n",
        "summary_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "698TfojYisSe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/earthquake_data/NepalEarhquakeDamage2015.csv')\n",
        "\n",
        "# Final UI-safe features\n",
        "features = [\n",
        "    'age_building',\n",
        "    'count_floors_pre_eq',\n",
        "    'foundation_type',\n",
        "    'roof_type',\n",
        "    'ground_floor_type',\n",
        "    'position',\n",
        "    'land_surface_condition'\n",
        "]\n",
        "\n",
        "target = 'damage_grade'\n",
        "\n",
        "df = df[features + [target]]\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encode target using LabelEncoder\n",
        "le_ui = LabelEncoder() # Create a new LabelEncoder for the UI model\n",
        "df[target] = le_ui.fit_transform(df[target])\n",
        "\n",
        "# One-hot encode categorical features\n",
        "X = pd.get_dummies(df[features])\n",
        "y = df[target]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "num_cols = ['age_building', 'count_floors_pre_eq']\n",
        "\n",
        "# Impute\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "X_train[num_cols] = num_imputer.fit_transform(X_train[num_cols])\n",
        "X_test[num_cols] = num_imputer.transform(X_test[num_cols])\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
        "\n",
        "# Train LightGBM model\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=31,\n",
        "    random_state=42\n",
        ")\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Train XGBoost model with the same reduced feature set\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=300, # Reduced for faster training\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False, # Suppress warning\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# SAVE EVERYTHING\n",
        "joblib.dump(lgb_model, 'lgb_model.joblib')\n",
        "joblib.dump(xgb_model, 'xgb_model.joblib') # Save the new XGBoost model\n",
        "joblib.dump(X.columns.tolist(), 'features.joblib')\n",
        "joblib.dump(num_imputer, 'num_imputer.joblib')\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "joblib.dump(le_ui, 'label_encoder.joblib') # Save the LabelEncoder used here\n",
        "\n",
        "print(\"Models, preprocessors, and features.joblib created successfully with UI-safe features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqay0TJNWFdw"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from prediction import predict_damage\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Earthquake Damage Predictor\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# ---- Constants ----\n",
        "MODEL_ACCURACY = 0.76  # 76%\n",
        "\n",
        "def confidence_to_certainty(conf):\n",
        "    if conf < 0.35:\n",
        "        return \"Low\"\n",
        "    elif conf < 0.50:\n",
        "        return \"Moderate\"\n",
        "    elif conf < 0.65:\n",
        "        return \"High\"\n",
        "    else:\n",
        "        return \"Very High\"\n",
        "\n",
        "# ---- Sidebar ----\n",
        "st.sidebar.header(\"Input Parameters\")\n",
        "st.sidebar.write(\"Provide the building details for prediction.\")\n",
        "\n",
        "with st.sidebar.form(\"input_form\"):\n",
        "    st.subheader(\"Structural Features\")\n",
        "    age = st.number_input(\"Building Age (years)\", 0, 200, 20)\n",
        "    floors = st.number_input(\"Number of Floors\", 1, 10, 2)\n",
        "\n",
        "    st.subheader(\"Construction Details\")\n",
        "    foundation = st.selectbox(\"Foundation Type\", ['mud', 'cement', 'other'])\n",
        "    roof = st.selectbox(\"Roof Type\", ['bamboo', 'metal', 'concrete', 'other'])\n",
        "    ground = st.selectbox(\"Ground Floor Type\", ['mud', 'cement', 'other'])\n",
        "\n",
        "    st.subheader(\"Position & Land\")\n",
        "    position = st.selectbox(\"Building Position\", ['attached', 'not_attached'])\n",
        "    land_surface = st.selectbox(\"Land Surface Condition\", ['flat', 'slope', 'other'])\n",
        "\n",
        "    submit = st.form_submit_button(\"Predict\")\n",
        "\n",
        "# ---- Main Panel ----\n",
        "st.title(\"Earthquake Building Damage Prediction\")\n",
        "st.write(\"Predict potential building damage based on structural characteristics.\")\n",
        "\n",
        "if submit:\n",
        "    input_df = pd.DataFrame([{\n",
        "        'age': age,\n",
        "        'count_floors_pre_eq': floors,\n",
        "        'foundation_type': foundation,\n",
        "        'roof_type': roof,\n",
        "        'ground_floor_type': ground,\n",
        "        'position': position,\n",
        "        'land_surface_condition': land_surface\n",
        "    }])\n",
        "\n",
        "    label, confidence = predict_damage(input_df)\n",
        "    certainty = confidence_to_certainty(confidence)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"Prediction Result\")\n",
        "\n",
        "    # Display main prediction\n",
        "    st.success(f\"Predicted Damage Level: **{label.upper()}**\")\n",
        "\n",
        "    # Additional information in columns for clarity\n",
        "    col1, col2 = st.columns(2)\n",
        "    col1.write(f\"**Model Certainty:** {certainty}\")\n",
        "    col2.write(f\"**Overall Model Accuracy:** {int(MODEL_ACCURACY * 100)}%\")\n",
        "\n",
        "    # Optional technical details\n",
        "    with st.expander(\"See Technical Details\"):\n",
        "        st.write(f\"Raw prediction confidence: **{confidence:.2%}**\")\n",
        "        st.caption(\n",
        "            \"Note: In multi-class damage prediction, moderate confidence is expected due to overlapping damage categories.\"\n",
        "        )\n",
        "\n",
        "# ---- Footer ----\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"All predictions are based on model estimation. Use as a guide, not a definitive assessment.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dcf8b84"
      },
      "outputs": [],
      "source": [
        "%%writefile prediction.py\n",
        "\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess_input(df_raw, training=False):\n",
        "    df = df_raw.copy()\n",
        "\n",
        "    # Load all preprocessors and column lists\n",
        "    scaler = joblib.load('scaler.joblib')\n",
        "    num_imputer = joblib.load('num_imputer.joblib')\n",
        "\n",
        "    # The features.joblib saved in nOkF82Xv2mKV is X.columns.tolist() which are the FINAL OHE features for the UI-safe model.\n",
        "    final_model_features = joblib.load('features.joblib') # Renaming this for clarity\n",
        "\n",
        "    # --- Manual definition of numerical columns for the UI-safe model ---\n",
        "    # These are the columns the num_imputer and scaler were actually fitted on in nOkF82Xv2mKV\n",
        "    num_cols_ui_safe = ['age_building', 'count_floors_pre_eq']\n",
        "\n",
        "    # --- Step 1: Align input column names ---\n",
        "    if 'age' in df.columns:\n",
        "        df.rename(columns={'age': 'age_building'}, inplace=True)\n",
        "\n",
        "    # --- Step 2: Extract and preprocess numerical features ---\n",
        "    # Create a temporary DataFrame for numerical features from the UI input\n",
        "    # Initialize with 0s and then fill if available in df\n",
        "    df_numerical_subset = pd.DataFrame(0, index=df.index, columns=num_cols_ui_safe)\n",
        "    for col in num_cols_ui_safe:\n",
        "        if col in df.columns:\n",
        "            df_numerical_subset[col] = df[col]\n",
        "\n",
        "    # Apply numerical imputation and scaling to this subset\n",
        "    df_numerical_subset[num_cols_ui_safe] = num_imputer.transform(df_numerical_subset[num_cols_ui_safe])\n",
        "    df_numerical_subset[num_cols_ui_safe] = scaler.transform(df_numerical_subset[num_cols_ui_safe])\n",
        "\n",
        "    # --- Step 3: Handle Categorical Features (One-Hot Encoding) ---\n",
        "    # Identify categorical columns from the raw input that are NOT numerical\n",
        "    # This assumes `df` contains the raw categorical inputs like 'foundation_type', 'roof_type' etc.\n",
        "    categorical_raw_cols_from_ui = [col for col in df.columns if col not in num_cols_ui_safe and df[col].dtype == 'object']\n",
        "    df_processed_categorical = pd.get_dummies(df[categorical_raw_cols_from_ui], drop_first=True)\n",
        "\n",
        "    # --- Step 4: Combine numerical and categorical features ---\n",
        "    # Combine the processed numerical and categorical features\n",
        "    processed_features = pd.concat([df_numerical_subset, df_processed_categorical], axis=1)\n",
        "\n",
        "    # --- Step 5: Final column alignment with the model's training features ---\n",
        "    # Create a final DataFrame with all columns from final_model_features, filled with 0 if missing\n",
        "    final_df = pd.DataFrame(0, index=processed_features.index, columns=final_model_features)\n",
        "    for col in final_model_features:\n",
        "        if col in processed_features.columns:\n",
        "            final_df[col] = processed_features[col]\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def predict_damage(input_df):\n",
        "    model = joblib.load('lgb_model.joblib')\n",
        "    label_encoder = joblib.load('label_encoder.joblib')\n",
        "\n",
        "    processed = preprocess_input(input_df)\n",
        "    pred = model.predict(processed)\n",
        "    prob = model.predict_proba(processed)\n",
        "\n",
        "    damage_label = label_encoder.inverse_transform(pred)[0]\n",
        "    confidence = prob.max()\n",
        "\n",
        "    return damage_label, confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhNosL8VzVTK"
      },
      "outputs": [],
      "source": [
        "from prediction import preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELF19R_ZeaFg"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip -q install pyngrok streamlit > /dev/null 2>&1\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import getpass\n",
        "import subprocess\n",
        "\n",
        "\n",
        "auth_token = getpass.getpass(\"Enter your ngrok authtoken (or press Enter if set below): \")\n",
        "if not auth_token.strip():\n",
        "\n",
        "    auth_token = \"36VH5tP1L1rSZvhq3LXCr6rLauL_4xBtyvUjAj4BRhg8zWTjV\"\n",
        "\n",
        "ngrok.set_auth_token(auth_token)\n",
        "\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7196bfb6"
      },
      "source": [
        "#### Data Distribution: Building Ages (Histogram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6786f53"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['age_building'], bins=30, kde=True)\n",
        "plt.title('Distribution of Building Ages')\n",
        "plt.xlabel('Building Age (years)')\n",
        "plt.ylabel('Number of Buildings')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28137122"
      },
      "source": [
        "#### Data Distribution: Foundation Types (Bar Chart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b2b1e79"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(y=df['foundation_type'], order=df['foundation_type'].value_counts().index)\n",
        "plt.title('Distribution of Foundation Types')\n",
        "plt.xlabel('Number of Buildings')\n",
        "plt.ylabel('Foundation Type')\n",
        "plt.grid(axis='x', alpha=0.75)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e8015f8"
      },
      "source": [
        "#### Relationship: Building Age vs. Number of Floors (Scatter Plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a7acbd5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=df['age_building'], y=df['count_floors_pre_eq'], alpha=0.6)\n",
        "plt.title('Relationship between Building Age and Number of Floors')\n",
        "plt.xlabel('Building Age (years)')\n",
        "plt.ylabel('Number of Floors')\n",
        "plt.grid(True, alpha=0.75)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89c04619"
      },
      "source": [
        "#### Model Performance: Confusion Matrix (LightGBM)\n",
        "\n",
        "This confusion matrix will show you how many instances of each true damage grade were predicted as each possible damage grade. The diagonal elements represent correct predictions, while off-diagonal elements show misclassifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b699ba9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import joblib\n",
        "\n",
        "\n",
        "lgb_clf = joblib.load('lgb_model.joblib')\n",
        "le_ui = joblib.load('label_encoder.joblib')\n",
        "\n",
        "\n",
        "y_pred_lgbm = lgb_clf.predict(X_test)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_lgbm)\n",
        "\n",
        "\n",
        "damage_labels = le_ui.inverse_transform(sorted(y_test.unique()))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=damage_labels, yticklabels=damage_labels)\n",
        "plt.title('Confusion Matrix for LightGBM Model')\n",
        "plt.xlabel('Predicted Damage Grade')\n",
        "plt.ylabel('True Damage Grade')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc7484ac"
      },
      "source": [
        "#### Model Performance: Confusion Matrix (XGBoost)\n",
        "\n",
        "This confusion matrix shows how many instances of each true damage grade were predicted as each possible damage grade for the XGBoost model. The diagonal elements represent correct predictions, while off-diagonal elements show misclassifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38c61f52"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import joblib\n",
        "\n",
        "# Load the trained XGBoost model and label encoder\n",
        "xgb_clf = joblib.load('xgb_model.joblib')\n",
        "le_ui = joblib.load('label_encoder.joblib')\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
        "\n",
        "# Get damage labels for plotting\n",
        "damage_labels = le_ui.inverse_transform(sorted(y_test.unique()))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=damage_labels, yticklabels=damage_labels)\n",
        "plt.title('Confusion Matrix for XGBoost Model')\n",
        "plt.xlabel('Predicted Damage Grade')\n",
        "plt.ylabel('True Damage Grade')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec6de3c3"
      },
      "source": [
        "## Expected Outputs of the Streamlit Application\n",
        "\n",
        "Upon successful submission of building parameters, the Streamlit application is designed to provide clear and actionable insights into the predicted earthquake damage level. The output section will dynamically update to display the model's prediction along with confidence measures.\n",
        "\n",
        "### Output Structure:\n",
        "\n",
        "1.  **Prediction Result Header**: A prominent header indicating the section for prediction results.\n",
        "\n",
        "2.  **Predicted Damage Level**: The primary output, displayed prominently, will be the predicted damage grade for the input building. This will be shown as:\n",
        "    *   **`Predicted Damage Level: [GRADE]`** (e.g., `Predicted Damage Level: GRADE 3`)\n",
        "    \n",
        "3.  **Confidence and Accuracy Metrics**: Presented in a columnar layout for readability, these metrics provide context to the prediction:\n",
        "    *   **Model Certainty**: An qualitative assessment of the model's confidence in its prediction, categorized as:\n",
        "        *   `Low` (confidence < 35%)\n",
        "        *   `Moderate` (35% <= confidence < 50%)\n",
        "        *   `High` (50% <= confidence < 65%)\n",
        "        *   `Very High` (confidence >= 65%)\n",
        "    *   **Overall Model Accuracy**: A static display of the general accuracy of the underlying machine learning model, typically shown as a percentage (e.g., `Overall Model Accuracy: 76%`).\n",
        "\n",
        "4.  **Technical Details (Expandable)**: An expandable section (`st.expander`) provides deeper insights for users interested in the model's raw output:\n",
        "    *   **Raw prediction confidence**: The exact probability score (as a percentage) that the model assigns to the predicted damage grade (e.g., `Raw prediction confidence: 72.50%`).\n",
        "    *   **Note**: A brief explanation about why moderate confidence is expected in multi-class damage prediction due to overlapping categories.\n",
        "\n",
        "### Example Output (Illustrative):\n",
        "\n",
        "```\n",
        "---\n",
        "### Prediction Result\n",
        "\n",
        " Predicted Damage Level: **GRADE 3**\n",
        "\n",
        "| Model Certainty    | Overall Model Accuracy |\n",
        "|--------------------|------------------------|\n",
        "| **Very High**      | **76%**                |\n",
        "\n",
        "<details>\n",
        "<summary>See Technical Details</summary>\n",
        "\n",
        "Raw prediction confidence: **78.23%**\n",
        "Note: In multi-class damage prediction, moderate confidence is expected due to overlapping damage categories.\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "All predictions are based on model estimation. Use as a guide, not a definitive assessment.\n",
        "```\n",
        "\n",
        "This structure ensures that users receive both an easy-to-understand summary and detailed information about the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "540be45f"
      },
      "source": [
        "## Model Evaluation Metrics\n",
        "\n",
        "This section details the performance of the trained classification models (XGBoost and LightGBM) on the test dataset. The primary goal is to assess their ability to accurately predict the damage grade of buildings after an earthquake.\n",
        "\n",
        "### Metrics Used:\n",
        "\n",
        "*   **Accuracy**: The proportion of correctly classified instances.\n",
        "*   **Precision**: The ability of the classifier not to label as positive a sample that is negative (i.e., fewer false positives).\n",
        "*   **Recall**: The ability of the classifier to find all the positive samples (i.e., fewer false negatives).\n",
        "*   **F1-Score**: The harmonic mean of precision and recall, providing a single metric that balances both.\n",
        "*   **Support**: The number of actual occurrences of the class in the specified dataset.\n",
        "*   **Macro Average**: Calculates metrics for each label, and finds their unweighted mean.\n",
        "*   **Weighted Average**: Calculates metrics for each label, and finds their average weighted by support.\n",
        "\n",
        "### Results:\n",
        "\n",
        "#### XGBoost Classifier\n",
        "\n",
        "*   **Overall Accuracy**: 0.7552\n",
        "\n",
        "| Damage Grade | Precision | Recall | F1-Score | Support |\n",
        "|--------------|-----------|--------|----------|---------|\n",
        "| Grade 0      | 0.93      | 0.85   | 0.89     | 15763   |\n",
        "| Grade 1      | 0.53      | 0.44   | 0.48     | 17452   |\n",
        "| Grade 2      | 0.49      | 0.41   | 0.45     | 27284   |\n",
        "| Grade 3      | 0.63      | 0.82   | 0.71     | 36769   |\n",
        "| Grade 4      | 1.00      | 0.95   | 0.98     | 55154   |\n",
        "| **Macro Avg**| 0.72      | 0.70   | 0.70     | 152422  |\n",
        "| **Weighted Avg**| 0.76      | 0.76   | 0.75     | 152422  |\n",
        "\n",
        "#### LightGBM Classifier\n",
        "\n",
        "*   **Overall Accuracy**: 0.7655\n",
        "\n",
        "| Damage Grade | Precision | Recall | F1-Score | Support |\n",
        "|--------------|-----------|--------|----------|---------|\n",
        "| Grade 0      | 0.92      | 0.87   | 0.89     | 15763   |\n",
        "| Grade 1      | 0.54      | 0.46   | 0.50     | 17452   |\n",
        "| Grade 2      | 0.51      | 0.45   | 0.48     | 27284   |\n",
        "| Grade 3      | 0.65      | 0.81   | 0.72     | 36769   |\n",
        "| Grade 4      | 1.00      | 0.96   | 0.98     | 55154   |\n",
        "| **Macro Avg**| 0.73      | 0.71   | 0.71     | 152422  |\n",
        "| **Weighted Avg**| 0.77      | 0.77   | 0.76     | 152422  |\n",
        "\n",
        "### Summary and Comparison:\n",
        "\n",
        "Both models achieved good overall accuracy. The LightGBM classifier showed slightly higher overall accuracy (0.7655) compared to XGBoost (0.7552) on this dataset. Both models performed exceptionally well in identifying 'Grade 4' damage (severe damage) with high precision and recall. However, 'Grade 1' and 'Grade 2' damage predictions show lower precision and recall, indicating more difficulty in distinguishing between these categories. The weighted average metrics, which account for class imbalance, are generally higher than the macro averages, reflecting the models' stronger performance on the more prevalent damage grades.\n",
        "\n",
        "For future improvements, focusing on the classification of 'Grade 1' and 'Grade 2' could significantly enhance the model's overall F1-score and its utility for nuanced damage assessment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EO8bPUK5Gyw"
      },
      "source": [
        "Median Imputation: \"I used median imputation because it's good for handling missing values in numerical data without being heavily affected by extreme values (outliers), which could happen if I used the mean.\"\n",
        "\n",
        "StandardScaler: \"I used StandardScaler to make sure all my numerical features have a similar scale. This is important because many machine learning models, like the ones I used, can get confused if one feature has very large numbers and another has very small numbers.\"\n",
        "\n",
        "IQR Clipping (Outlier Handling): \"I used IQR (Interquartile Range) clipping to handle outliers, which are unusual data points. This method helps to limit very high or very low values, making sure they don't unfairly influence the model,\n",
        "which might otherwise learn from noise instead of real patterns.\"\n",
        "\n",
        "LightGBM vs. XGBoost: \"LightGBM performed slightly better than XGBoost in my tests. This could be because LightGBM is generally faster and often performs well on large datasets, and it might have found slightly better patterns in this particular dataset.\" (You can also mention its 'leaf-wise' growth strategy as a technical detail if you feel comfortable).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "By analyzing feature importance, we can see which building characteristics, such as the age_building or foundation_type, had the biggest impact on predicting damage. This helps us understand what factors are most critical for a building's resilience to earthquakes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F_a-gCG5Kh1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}